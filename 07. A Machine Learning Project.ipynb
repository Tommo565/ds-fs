{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. A Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Machine Learning in SKL & Tensorflow (pdf)](./docs/Hands.Machine.Learning.Scikit.Learn.Tensorflow.5225.pdf#page=58)<br/>\n",
    "[Machine Learning in SKL & Tensorflow (Repo)](https://github.com/ageron/handson-ml)<br/>\n",
    "[Matplotlib Colormaps](https://matplotlib.org/users/colormaps.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling as pdpf\n",
    "from six.moves import urllib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checklist**  \n",
    "\n",
    "The basic steps you will go through when taking on an ML project are as follows:  \n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Frame the Problem and Look at the Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question to ask is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit from this model? This is important\n",
    "because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    "\n",
    "In this case we're going to build a model to predict a district’s median housing price. This will be **Pipelined** into another Machine Learning system, along with many other signals.\n",
    "This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue.  \n",
    "\n",
    "The next question to ask is what the current solution looks like (if any). It will often give you a reference performance, as well as insights on how to solve the problem.\n",
    "\n",
    "Then, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\n",
    "\n",
    "In this case, we have a typical supervised learning task since we are given labeled training examples (each instance comes with the expected output, i.e., the district’s median\n",
    "housing price). Moreover, it is also a typical regression task, since you are asked to predict a value. More specifically, this is a multivariate regression problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). Previously, you predicted life satisfaction based on just one feature, the GDP per capita, so it was a univariate regression problem. Finally, there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.\n",
    "\n",
    "**Pipelines**\n",
    "\n",
    "A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply. Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store, and then some time later the next component in the pipeline pulls this data and spits out its own output, and so on. \n",
    "\n",
    "Each component is fairly self-contained: the interface between components is simply the data store. This makes the system quite simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust. On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overallsystem’s performance drops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting a Performance Measure**  \n",
    "\n",
    "Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It measures the standard deviation of the errors the\n",
    "system makes in its predictions. For example, an RMSE equal to 50,000 means that about 68% of the system’s predictions fall within \\$50,000 of the actual value, and about 95% of the predictions fall within \\$100,000 of the actual value.  \n",
    "\n",
    "The formula for RMSE is as follows:  \n",
    "\n",
    "<span style=\"color:#888888\">\n",
    "    ${\\displaystyle \n",
    "        RMSE (\\textbf{X}, h) = \\sqrt{ \n",
    "            {\\frac {1}{m} } \n",
    "            {\\sum_{i=1}^m }\n",
    "            (h(\\textbf{x})^{(i)} -\n",
    "            y^{(i)})^{2}\n",
    "        }\n",
    "    }$\n",
    "</span>\n",
    "\n",
    "Where:\n",
    "\n",
    "<span style=\"color:#888888\">\n",
    "$RMSE (\\textbf{X}, h)$ = is the cost function measured on the set of examples using your hypothesis $h$.  \n",
    "$X$ = Matrix containing all the feature values (excluding labels) of all instances in the dataset  \n",
    "$h$ = System’s prediction function, also called a *hypothesis*  \n",
    "$m$ = Number of instances in the dataset  \n",
    "$x^{(i)}$ = Vector of all the feature values (excluding the label) of the $i$th instance in the dataset  \n",
    "$y^{(y)}$ = Vector of all the feature values (excluding the label) of the $y$th instance in the dataset  \n",
    "</span>\n",
    "\n",
    "Lowercase italic font is used for for scalar values (such as $m$ or $y^{i}$ ) and function names (such as $h$), lowercase bold font for vectors (such as ${\\textbf x^{(i)} }$), and uppercase bold font for matrices (such as $\\textbf X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Assumptions**  \n",
    "\n",
    "Lastly, it is good practice to list and verify the assumptions that were made so far (by you or others); this can catch serious issues early on.  \n",
    "\n",
    "For example, the district prices that your system outputs are going to be fed into a downstream Machine Learning system, and we assume that these prices are going to be used as such. But what if the downstream system actually converts the prices into categories (e.g., “cheap,” “medium,” or “expensive”) and then uses those categories instead of the prices themselves? In this case, getting the price perfectly right is not important at all; your system just needs to get the category right. If that’s so, then the problem should have been framed as a classification task, not a regression task. You don’t want to find this out after working on a regression system for months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Questions to ask of the data**\n",
    "\n",
    "* How was it gathered?\n",
    "* Is it a sample or a full population?\n",
    "* What pre-processing, if any, has the data undergone? Are there any other variables missing?\n",
    "* If it's currently used, what is it used for?\n",
    "* Are there any schema or descriptions available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/housing.csv')       # Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Exploration\n",
    "\n",
    "print(df.info())\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    alpha=0.4,\n",
    "    s=df[\"population\"]/100, \n",
    "    label=\"population\",\n",
    "    c=\"median_house_value\", \n",
    "    cmap=plt.get_cmap(\"hot\"), \n",
    "    colorbar=True,\n",
    "    figsize=(16,12)\n",
    ")\n",
    "plt.legend()    # Creating a geo plot of the lat/lon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdpf.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**  \n",
    "\n",
    "* Median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.  \n",
    "\n",
    "* The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels) and Your algorithms may learn that prices never go beyond that limit.  \n",
    "\n",
    "* These attributes have very different scales.\n",
    "\n",
    "* Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n",
    "\n",
    "* Households is highly correlated with Population\n",
    "\n",
    "* Total Bedrooms is highly correlated with Total Rooms   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most median income values are clustered around 2–5 (tens of thousands of dollars), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an income_cat variable\n",
    "\n",
    "df[\"income_cat\"] = np.ceil(df[\"median_income\"] / 1.5)\n",
    "df[\"income_cat\"].where(df[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)     # Creating a train / test split\n",
    "df = train                                                             # Assigning the train set as the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should make sure that the income_cat variable is fairly represented in both the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"income_cat\"].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Correlation**  \n",
    "\n",
    "We can use pandas scatter_matrix function to check for correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(df[attributes], figsize=(16, 12), s=4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median_house_value by median_income plot reveals a few things.  \n",
    "\n",
    "First, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500k. But this plot reveals other less obvious straight lines: a horizontal line around 450k, another around 350k, perhaps one around 280k, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some more meaningful variables to the dataset\n",
    "\n",
    "df[\"rooms_per_household\"] = df[\"total_rooms\"]/df[\"households\"]\n",
    "df[\"bedrooms_per_room\"] = df[\"total_bedrooms\"]/df[\"total_rooms\"]\n",
    "df[\"population_per_household\"]=df[\"population\"]/df[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
