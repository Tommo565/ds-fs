{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. A Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Machine Learning in SKL & Tensorflow (pdf)](./docs/Hands.Machine.Learning.Scikit.Learn.Tensorflow.5225.pdf#page=58)<br/>\n",
    "[Machine Learning in SKL & Tensorflow (Repo)](https://github.com/ageron/handson-ml)<br/>\n",
    "[Machine Learning in SKL & Tensorflox (Notebook)](https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb)<br/>\n",
    "[Matplotlib Colormaps](https://matplotlib.org/users/colormaps.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling as pdpf\n",
    "from six.moves import urllib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pandas.plotting import scatter_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checklist**  \n",
    "\n",
    "The basic steps you will go through when taking on an ML project are as follows:  \n",
    "1. Frame the problem and look at the big picture.\n",
    "2. Get the data.\n",
    "3. Explore the data to gain insights.\n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    "5. Explore many different models and short-list the best ones.\n",
    "6. Fine-tune your models and combine them into a great solution.\n",
    "7. Present your solution.\n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Frame the Problem and Look at the Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question to ask is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit from this model? This is important\n",
    "because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    "\n",
    "In this case we're going to build a model to predict a district’s median housing price. This will be **Pipelined** into another Machine Learning system, along with many other signals.\n",
    "This downstream system will determine whether it is worth investing in a given area or not. Getting this right is critical, as it directly affects revenue.  \n",
    "\n",
    "The next question to ask is what the current solution looks like (if any). It will often give you a reference performance, as well as insights on how to solve the problem.\n",
    "\n",
    "Then, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\n",
    "\n",
    "In this case, we have a typical supervised learning task since we are given labeled training examples (each instance comes with the expected output, i.e., the district’s median\n",
    "housing price). Moreover, it is also a typical regression task, since you are asked to predict a value. More specifically, this is a multivariate regression problem since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). Previously, you predicted life satisfaction based on just one feature, the GDP per capita, so it was a univariate regression problem. Finally, there is no continuous flow of data coming in the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.\n",
    "\n",
    "**Pipelines**\n",
    "\n",
    "A sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply. Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store, and then some time later the next component in the pipeline pulls this data and spits out its own output, and so on. \n",
    "\n",
    "Each component is fairly self-contained: the interface between components is simply the data store. This makes the system quite simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust. On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overallsystem’s performance drops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting a Performance Measure**  \n",
    "\n",
    "Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It measures the standard deviation of the errors the\n",
    "system makes in its predictions. For example, an RMSE equal to 50,000 means that about 68% of the system’s predictions fall within \\$50,000 of the actual value, and about 95% of the predictions fall within \\$100,000 of the actual value.  \n",
    "\n",
    "The formula for RMSE is as follows:  \n",
    "\n",
    "<span style=\"color:#888888\">\n",
    "    ${\\displaystyle \n",
    "        RMSE (\\textbf{X}, h) = \\sqrt{ \n",
    "            {\\frac {1}{m} } \n",
    "            {\\sum_{i=1}^m }\n",
    "            (h(\\textbf{x})^{(i)} -\n",
    "            y^{(i)})^{2}\n",
    "        }\n",
    "    }$\n",
    "</span>\n",
    "\n",
    "Where:\n",
    "\n",
    "<span style=\"color:#888888\">\n",
    "$RMSE (\\textbf{X}, h)$ = is the cost function measured on the set of examples using your hypothesis $h$.  \n",
    "$X$ = Matrix containing all the feature values (excluding labels) of all instances in the dataset  \n",
    "$h$ = System’s prediction function, also called a *hypothesis*  \n",
    "$m$ = Number of instances in the dataset  \n",
    "$x^{(i)}$ = Vector of all the feature values (excluding the label) of the $i$th instance in the dataset  \n",
    "$y^{(y)}$ = Vector of all the feature values (excluding the label) of the $y$th instance in the dataset  \n",
    "</span>\n",
    "\n",
    "Lowercase italic font is used for for scalar values (such as $m$ or $y^{i}$ ) and function names (such as $h$), lowercase bold font for vectors (such as ${\\textbf x^{(i)} }$), and uppercase bold font for matrices (such as $\\textbf X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the Assumptions**  \n",
    "\n",
    "Lastly, it is good practice to list and verify the assumptions that were made so far (by you or others); this can catch serious issues early on.  \n",
    "\n",
    "For example, the district prices that your system outputs are going to be fed into a downstream Machine Learning system, and we assume that these prices are going to be used as such. But what if the downstream system actually converts the prices into categories (e.g., “cheap,” “medium,” or “expensive”) and then uses those categories instead of the prices themselves? In this case, getting the price perfectly right is not important at all; your system just needs to get the category right. If that’s so, then the problem should have been framed as a classification task, not a regression task. You don’t want to find this out after working on a regression system for months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Questions to ask of the data**\n",
    "\n",
    "* How was it gathered?\n",
    "* Is it a sample or a full population?\n",
    "* What pre-processing, if any, has the data undergone? Are there any other variables missing?\n",
    "* If it's currently used, what is it used for?\n",
    "* Are there any schema or descriptions available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/housing.csv')       # Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Exploration\n",
    "\n",
    "print(df.info())\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    kind=\"scatter\", \n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    alpha=0.4,\n",
    "    s=df[\"population\"]/100, \n",
    "    label=\"population\",\n",
    "    c=\"median_house_value\", \n",
    "    cmap=plt.get_cmap(\"hot\"), \n",
    "    colorbar=True,\n",
    "    figsize=(16,12)\n",
    ")\n",
    "plt.legend()    # Creating a geo plot of the lat/lon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdpf.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**  \n",
    "\n",
    "* Median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher median incomes, and at 0.5 (actually 0.4999) for lower median incomes. Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.  \n",
    "\n",
    "* The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels) and Your algorithms may learn that prices never go beyond that limit.  \n",
    "\n",
    "* These attributes have very different scales.\n",
    "\n",
    "* Finally, many histograms are tail heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.\n",
    "\n",
    "* Households is highly correlated with Population\n",
    "\n",
    "* Total Bedrooms is highly correlated with Total Rooms   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro to SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn's API is very well designed and logical.\n",
    "\n",
    "* **Estimators**: Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator).\n",
    "* **Transformers**: Some estimators (such as an imputer) can also transform a dataset; these are called transformers.\n",
    "* **Predictors**: Finally, some estimators are capable of making predictions given a dataset; they are called predictors.  \n",
    "* **Inspection**: All the estimator’s hyperparameters are accessible directly via public instance variables.\n",
    "* **Nonproliferation of classes**: Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes.\n",
    "* **Composition**: Existing building blocks are reused as much as possible.\n",
    "* **Sensible defaults**: Scikit-Learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most median income values are clustered around 2–5 (tens of thousands of dollars), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an income_cat variable\n",
    "\n",
    "df[\"income_cat\"] = np.ceil(df[\"median_income\"] / 1.5)\n",
    "df[\"income_cat\"].where(df[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)     # Creating a train / test split\n",
    "df = train                                                             # Assigning the train set as the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should make sure that the income_cat variable is fairly represented in both the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"income_cat\"].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Correlation**  \n",
    "\n",
    "We can use pandas scatter_matrix function to check for correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(df[attributes], figsize=(20, 14), s=4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median_house_value by median_income plot reveals a few things.  \n",
    "\n",
    "First, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500k. But this plot reveals other less obvious straight lines: a horizontal line around 450k, another around 350k, perhaps one around 280k, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some more meaningful variables to the dataset\n",
    "\n",
    "df[\"rooms_per_household\"] = df[\"total_rooms\"]/df[\"households\"]\n",
    "df[\"bedrooms_per_room\"] = df[\"total_bedrooms\"]/df[\"total_rooms\"]\n",
    "df[\"population_per_household\"]=df[\"population\"]/df[\"households\"]\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio\n",
    "tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district — obviously the larger the houses, the more expensive they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a fresh copy of the training set\n",
    "\n",
    "df = train.drop(\"median_house_value\", axis=1)\n",
    "df_labels = train[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Numeric Variables**  \n",
    "\n",
    "Machine Learning algorithms do not cope well with missing data, so this needs to be dealt with. In dealing with missing data you have three options:  \n",
    "* Get rid of the column\n",
    "* Get rid of the row\n",
    "* Fill the value with another value (e.g. mean / median)  \n",
    "\n",
    "SKL has an `Imputer` class that helps us deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(\"ocean_proximity\", axis=1)      # Dropping non numeric vaues as Imputer won't deal with these\n",
    "imputer = Imputer(strategy=\"median\")              # Creating the imputer\n",
    "imputer.fit(df_copy)                              # Fit function replaces missing with the median data\n",
    "imputer.statistics_                               # imputer also creates a variable to store the imputed values\n",
    "df_imputed = imputer.transform(df_copy)           # Applies the imputer to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Categorical Variables**\n",
    "\n",
    "Earlier we left out the categorical attribute ocean_proximity because it is a text attribute so we cannot compute its median. Most Machine Learning algorithms prefer to work with numbers anyway, so let’s\n",
    "convert these text labels to numbers. Scikit-Learn provides a **transformer** for this task called LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()                          # Creating the Encoder\n",
    "df_cat = df[\"ocean_proximity\"]                    # Creating the category variable from the df\n",
    "df_cat_encoded = encoder.fit_transform(df_cat)    # Encoding the category \n",
    "print(df_cat_encoded)                             # The coded variable\n",
    "print(encoder.classes_)                           # The classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this representation is that MLalgorithms will assume that two nearby values are more similar than two distant values. Obviously this is not the case with categorical variables. To fix this issue, a common solution is to create one binary attribute per category with values of 0 & 1. This is called **one-hot encoding**, because only one attribute will be equal to 1 (hot), while the others will be 0 (cold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()                                 # Creating the One Hot Encoder\n",
    "cat = encoder.fit_transform(df_cat_encoded.reshape(-1,1)) # Reshaping the encoded categor\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a Scipy sparse matrix where 0's aren't recorded to save memory and increase efficiency. This can be converted to a numpy array using the .toarray() method.  \n",
    "\n",
    "We can create a one-shot one hot encoder using the LabelBinarizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()                         # Creating the one-shot one-hot Encoder\n",
    "cat = encoder.fit_transform(df[\"ocean_proximity\"]) # Encoding the category\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Transformers**  \n",
    "Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need is to create a class and implement three methods: fit() (returning self), transform(), and fit_transform(). You can get the last one for free by simply adding TransformerMixin as a base class. Also, if you add BaseEstimator as a base class (and avoid *args\n",
    "and \\**kwargs in your constructor) you will get two extra methods (get_params() and set_params()) that will be useful for automatic hyperparameter tuning. Below, there is a small transformer class\n",
    "that adds the combined attributes we discussed earlier.\n",
    "\n",
    "In this example the transformer has one hyperparameter, add_bedrooms_per_room, set to True by default (it is often helpful to provide sensible defaults). This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        \n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "\n",
    "# attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "# housing_extra_attribs = attr_adder.transform(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**  \n",
    "\n",
    "One of the most important transformations you need to apply to your data is **feature scaling**. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have\n",
    "very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required. There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.  \n",
    "\n",
    "**Min-max scaling** (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.  \n",
    "\n",
    "**Standardization** is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0 –15 down to 0 – 0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "\n",
    "**WARNING!** As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformation Pieplines**  \n",
    "\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations. Each subpipeline starts with a selector transformer: it simply transforms the data by selecting the desired attributes (numerical or categorical), dropping the rest, and converting the resulting DataFrame to a NumPy array.\n",
    "\n",
    "The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). The names can be anything you like.  \n",
    "\n",
    "When you call the pipeline’s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call, until it reaches the final estimator, for which it just calls the fit() method. The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler, which is a transformer, so the pipeline has a transform() method that applies all the transforms to the data in sequence (it also has a fit_transform method that we could have used instead of calling fit() and then transform()).\n",
    "\n",
    "You now have a pipeline for numerical values, and you also need to apply the LabelBinarizer on the categorical values: how can you join these transformations into a single pipeline? Scikit-Learn provides a FeatureUnion class for this. You give it a list of transformers (which can be entire transformer pipelines), and when its transform() method is called it runs each transformer’s transform() method\n",
    "in parallel, waits for their output, and then concatenates them and returns the result (and of course calling its fit() method calls all each transformer’s fit() method). A full pipeline handling both numerical and categorical attributes may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of what's been done so far\n",
    "\n",
    "df = pd.read_csv('./data/housing.csv')                             # Importing the data\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42) # Creating a train / test split\n",
    "\n",
    "train_labels = train[\"median_house_value\"].copy()                  # Creating a copy of the Median House Value\n",
    "train = train.drop(\"median_house_value\", axis=1)                   # Getting rid of Median House Value in the training set\n",
    "train_num = train.drop(\"ocean_proximity\", axis=1)                  # Dropping non numeric vaues as Imputer won't deal with these\n",
    "\n",
    "# Adding an Attributes adder class to add a hyperparameter\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        \n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "num_attribs = list(train_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Dataframe selector\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "        \n",
    "# Building the numeric pipeline        \n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Building the categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer', LabelBinarizer()),\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(train)\n",
    "housing_prepared.shape\n",
    "\n",
    "# Building the model\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've built our ML model we can now test it with some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/housing.csv')                          # Importing the da\n",
    "labels = df[\"median_house_value\"].copy()                        # Creating the labels dataset\n",
    "df = df.drop(\"median_house_value\", axis=1)                      # Getting rid of Median House Value in the training set\n",
    "\n",
    "\n",
    "some_data = df.iloc[:10]\n",
    "some_labels = labels.iloc[:10]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Actuals:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model isn't performing brilliantly. We can measure the RMSE using SKLs `mean_squared_error` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than nothing but clearly not a great score: most districts’ median_housing_values range between \\$120,000 and \\$265,000, so a typical prediction error of $68,628 is not very satisfying.\n",
    "This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful\n",
    "enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the\n",
    "model. This model is not regularized, so this rules out the last option. You could try to add more features (e.g., the log of the population), but first let’s try a more complex model to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want\n",
    "to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation. One way to evaluate the Decision Tree model would be to use the train_test_split function to split the training set into a smaller training set and a validation set, then train your models against the smaller\n",
    "training set and evaluate them against the validation set. It’s a bit of work, but nothing too difficult and it would work fairly well. A great alternative is to use Scikit-Learn’s cross-validation feature. The following code performs K-fold cross-validation: it randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse_scores = cross_val_score(tree_reg, housing_prepared, train_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-tree_rmse_scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    \n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, train_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rmse_scores = cross_val_score(forest_reg, housing_prepared, train_labels,\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "rmse_scores = np.sqrt(-forest_rmse_scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. However, before you dive much deeper in Random Forests, you should try out many other models from various categories of Machine Learning algorithms (several Support Vector Machines with different kernels, possibly a neural network, etc.), without spending too much time tweaking the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a set of promising models you have a number of ways in which these can be fine tuned as follows:\n",
    "\n",
    "* Grid Search\n",
    "* Randomised Search\n",
    "* Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**\n",
    "\n",
    "One way to fine tune would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to\n",
    "explore many combinations. Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict (don’t worry about what these\n",
    "hyperparameters mean for now; they will be explained in Chapter 7), then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to\n",
    "False instead of True (which is the default value for this hyperparameter). All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor\n",
    "hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "        print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we obtain the best solution by setting the max_features hyperparameter to 6, and the n_estimators hyperparameter to 30. The RMSE score for this combination is 49,959, which is slightly\n",
    "better than the score you got earlier using the default hyperparameter values (which was 52,634). Congratulations, you have successfully fine-tuned your best model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomized Search**\n",
    "\n",
    "The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use\n",
    "RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations\n",
    "by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "\n",
    "* If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the\n",
    "grid search approach).  \n",
    "* You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Methods**  \n",
    "\n",
    "Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform\n",
    "better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze the Best Models and Their Errors**  \n",
    "\n",
    "You will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.active_features_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, you may want to try dropping some of the less useful features. You should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem (adding extra features or, on the contrary, getting rid of uninformative ones, cleaning up outliers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Your System on the Test Set**  \n",
    "\n",
    "After tweaking your models for a while, you eventually have a system that performs sufficiently well. Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your full_pipeline to transform the data (call transform(), not fit_transform()!), and evaluate the final model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = test.drop(\"median_house_value\", axis=1)\n",
    "y_test = test[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance will usually be slightly worse than what you measured using cross-validation if you did a lot of hyperparameter tuning (because your system ends up fine-tuned to perform well on the validation data, and will likely not perform as well on unknown datasets). It is not the case in this example, but when this happens you must resist the temptation to tweak the hyperparameters to make the numbers look good on the test set; the improvements would be unlikely to generalize to new data. Now comes the project prelaunch phase: you need to present your solution (highlighting what you have\n",
    "learned, what worked and what did not, what assumptions were made, and what your system’s limitations are), document everything, and create nice presentations with clear visualizations and easy-to-remember statements (e.g., “the median income is the number one predictor of housing prices”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch, Monitor, and Maintain Your System**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, you got approval to launch! You need to get your solution ready for production, in particular by plugging the production input data sources into your system and writing tests. You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. This is important to catch not only sudden breakage, but also performance degradation. This is quite common because models tend to “rot” as data evolves over time, unless the models are regularly trained on fresh data.\n",
    "\n",
    "\n",
    "Evaluating your system’s performance will require sampling the system’s predictions and evaluating them. This will generally require a human analysis. These analysts may be field experts, or workers on a\n",
    "crowdsourcing platform (such as Amazon Mechanical Turk or CrowdFlower). Either way, you need to plug the human evaluation pipeline into your system. You should also make sure you evaluate the system’s input data quality. Sometimes performance will degrade slightly because of a poor quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the inputs is particularly important for online learning systems.\n",
    "\n",
    "\n",
    "Finally, you will generally want to train your models on a regular basis using fresh data. You should automate this process as much as possible. If you don’t, you are very likely to refresh your model only every six months (at best), and your system’s performance may fluctuate severely over time. If your system is an online learning system, you should make sure you save snapshots of its state at regular intervals so you can easily roll back to a previously working state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Project Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre Project**\n",
    "\n",
    "1. What's the business issue? Is it a Machine Learning problem?  \n",
    "2. What does the current solution look like, if indeed there is one?\n",
    "3. What does the data look like? Some ideas:\n",
    "    * How was it gathered?  \n",
    "    * Is it a sample or a full population?  \n",
    "    * What pre-processing, if any, has the data undergone? Are there any other variables missing?  \n",
    "    * If it's currently used, what is it used for?  \n",
    "    * Are there any schema or descriptions available?  \n",
    "4. How do you anticipate solving the problem? Should you use supervised, unsupervised, or reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques?  \n",
    "5. What Performance Measure will you use?\n",
    "6. What assumptions have been made, either by you or others?\n",
    "\n",
    "\n",
    "\n",
    "**Data Preparation** \n",
    "\n",
    "1. Perform some basic exploratory analysis of the data. (missing values, column names, cardinality, correlations, categorical / numeric variables, overall cleanliness etc.)\n",
    "2. Visualise the data. (spread & distribution, patterns, pre-processing etc.) \n",
    "3. Eyeball a sample of the data. Are there any glaring issues (e.g. user error, sample_bias, consistency etc.)\n",
    "4. Performing averaging, combination or dimensionality reduction, creating categorical variables (e.g. simplification of the dataset)\n",
    "\n",
    "**Pipelining**  \n",
    "\n",
    "1. Deal with missing values (SKL Imputer)\n",
    "2. Encode Categorical Variables (SKL Label Encoder / One Hot Encoder)\n",
    "3. Consider Feature Scaling if the variables have vastly different scales  \n",
    "4. Consider adding hyperparameter options based upon the variables in the dataset.\n",
    "\n",
    "**Building / Running the model**\n",
    "\n",
    "1. What's the error?\n",
    "2. Is the model overfitting? \n",
    "3. Is the model underfitting?\n",
    "4. Consider trying different model types.\n",
    "\n",
    "**Fine Tuning**  \n",
    "\n",
    "1. Consider Grid Search, Randomised Search or Ensemble Methods\n",
    "2. Analyse the best models and their errors. What variables can we remove?\n",
    "3. Evaluate on the test set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
